{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dir = r'C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgs_mod_dir = os.path.join(mod_dir,'hgs')\n",
    "model_name = 'lys'\n",
    "grok_file_path = os.path.join(hgs_mod_dir,model_name + '_e.grok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_file_stem = model_name + '_e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCoupledModelDir(mod_dir):\n",
    "    # Create directories if they don't exist\n",
    "    coupled_mod_dir = os.path.join(mod_dir,'coupled')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_dir)\n",
    "    except:\n",
    "        print(coupled_mod_dir + ' already exists')\n",
    "    # Create hgs subdirectory\n",
    "    coupled_mod_hgs_dir = os.path.join(coupled_mod_dir,'hgs')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_hgs_dir)\n",
    "    except:\n",
    "        print(coupled_mod_hgs_dir + ' already exists')\n",
    "    # Create dssat subdirectory\n",
    "    coupled_mod_dssat_dir = os.path.join(coupled_mod_dir,'dssat')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_dssat_dir)\n",
    "    except:\n",
    "        print(coupled_mod_dssat_dir + ' already exists')\n",
    "    return coupled_mod_dir,coupled_mod_hgs_dir,coupled_mod_dssat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpinUpHeadsOutputFile(hgs_mod_dir,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    # Identify spin up grok name\n",
    "    heads_file_stem = grok_file_stem.split('_')[0] + '_suo.head_pm'\n",
    "    # Get highest number head_pm output file\n",
    "    heads_file = [x for x in os.listdir(hgs_mod_dir) if heads_file_stem in x][-1]\n",
    "    # Get path of that file\n",
    "    spin_up_heads_file_path = os.path.join(hgs_mod_dir,heads_file)\n",
    "    # Get path of coupled model hgs dir\n",
    "    coupled_spin_up_heads_file_path = os.path.join(coupled_mod_hgs_dir,heads_file)\n",
    "    # Copy file\n",
    "    shutil.copy(spin_up_heads_file_path,coupled_spin_up_heads_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHGSPropsFiles(hgs_mod_dir,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    # Get model name\n",
    "    model_name = grok_file_stem.split('_')[0]\n",
    "    # Get file names\n",
    "    mprops_name = model_name + '.mprops'\n",
    "    oprops_name = model_name + '.oprops'\n",
    "    etprops_name = model_name + '.etprops'\n",
    "    for file in [mprops_name,oprops_name,etprops_name]:\n",
    "        # Paths and copy\n",
    "        shutil.copy(os.path.join(hgs_mod_dir,file),os.path.join(coupled_mod_hgs_dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStandaloneGrokLines(grok_file_path):\n",
    "    # Read grok file\n",
    "    with open(grok_file_path,'r') as file_in:\n",
    "        lines = file_in.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetStandaloneGrokPrecSeries(standalone_grok_lines):\n",
    "    # Get start index\n",
    "    start = standalone_grok_lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    end = standalone_grok_lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Get P Block\n",
    "    plines = standalone_grok_lines[start:end]\n",
    "    # Get p series\n",
    "    start = plines.index('    time value table\\n')\n",
    "    end = plines.index('    end\\n')\n",
    "    # Get time series lines\n",
    "    tslines = plines[start+1:end]\n",
    "    # Get Daily P Series\n",
    "    p = [float(x.split(' ')[5].strip()) for x in tslines]\n",
    "    # Get end day\n",
    "    end_day = len(p)\n",
    "    return p, end_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDailyCoupledGrokFileDay0(lines,day,p):\n",
    "    ## P Section\n",
    "    # Get start index\n",
    "    pstart = lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    pend = lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Build P entry\n",
    "    pentry = f'    time value table\\n    0.0 {P[day]:.2f}\\n    end\\n'\n",
    "    ## Output Section\n",
    "    # Get start index\n",
    "    ostart = lines.index('!!--Begin Output Times Section--\\n')\n",
    "    # Get end index\n",
    "    oend = lines.index('!!--End Output Times Section--\\n')\n",
    "    new_lines = lines[:pstart+1]+[pentry]+lines[pend:ostart+1]+['1.0\\nend\\n']+lines[oend:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDailyCoupledGrokFileDayN(lines,day,p,grok_file_stem):\n",
    "    ## IC Section\n",
    "    # Get start index\n",
    "    icstart = lines.index('!!--Begin Initial Head Section--\\n')\n",
    "    # Get end index\n",
    "    icend = lines.index('!!--End Initial Head Section--\\n')\n",
    "    # Build IC\n",
    "    icentry = '! Set initial heads from day n-1\\nchoose nodes all\\n\\ninitial head from output file\\n{0}day{1}o.head_pm.0001\\n\\nclear chosen nodes\\n'.format(grok_file_stem,day-1)\n",
    "    ## Flux Nodal Section\n",
    "    # Get start index\n",
    "    fnstart = lines.index('!!--Begin Flux Nodal for DSSAT ET Section--\\n')\n",
    "    # Get end index\n",
    "    fnend = lines.index('!!--End Flux Nodal for DSSAT ET Section--\\n')\n",
    "    # Build FN\n",
    "    fnentry = '! Set flux nodal to force DSSAT ET\\nboundary condition\\n    type\\n    flux nodal\\n\\n    node set\\n    coupled_section\\n\\n    time file table\\n    0.0 nflux.txt\\n    0.00069444 none\\n    end\\nend\\n'\n",
    "    ## P Section\n",
    "    # Get start index\n",
    "    pstart = lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    pend = lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Build P entry\n",
    "    pentry = f'    time value table\\n    0.0 {p[day]:.2f}\\n    end\\n'\n",
    "    ## Solute Transport IC Section\n",
    "    # Get start index\n",
    "    sticstart = lines.index('!!--Begin Solute Transport Initial Concentration Section--')\n",
    "    # Get end index\n",
    "    sticend = lines.index('!!--End Solute Transport Initial Concentration Section--')\n",
    "    # Build IC Entry\n",
    "    sticentry = '! NH4 and NO3 boundary initial concentrations from DSSAT model in root zone and hgs in non-coupled zone\\n\\nchoose nodes all\\n\\ninitial concentration from file\\niconc.txt\\n\\n'\n",
    "    ## Output Section\n",
    "    # Get start index\n",
    "    ostart = lines.index('!!--Begin Output Times Section--\\n')\n",
    "    # Get end index\n",
    "    oend = lines.index('!!--End Output Times Section--\\n')\n",
    "    new_lines = lines[:icstart+1]+[icentry]+lines[icend:fnstart+1]+[fnentry]+lines[fnend:pstart+1]+[pentry]+lines[pend:sticstart+1]+sticentry+lines[sticend:ostart+1]+['1.0\\nend\\n']+lines[oend:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteCoupledGrokFile(new_lines,day,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    new_grok_name = grok_file_stem + 'day{}'.format(day) + '.grok'\n",
    "    new_grok_path = os.path.join(coupled_mod_hgs_dir,new_grok_name)\n",
    "    with open(new_grok_path,'w') as file:\n",
    "        for entry in new_lines:\n",
    "            file.write(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled already exists\n",
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled\\hgs already exists\n",
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled\\dssat already exists\n"
     ]
    }
   ],
   "source": [
    "## Build Coupled Model\n",
    "# Create Directory Structure\n",
    "cmod_dir,cmod_hgs_dir,cmod_dssat_dir = CreateCoupledModelDir(mod_dir)\n",
    "# Copy over necessary hgs files\n",
    "GetSpinUpHeadsOutputFile(hgs_mod_dir,cmod_hgs_dir,grok_file_stem)\n",
    "GetHGSPropsFiles(hgs_mod_dir,cmod_hgs_dir,grok_file_stem)\n",
    "# Get standalone model grok lines and Prec series\n",
    "standalone_grok_lines = GetStandaloneGrokLines(grok_file_path)\n",
    "P, End_Day = GetStandaloneGrokPrecSeries(standalone_grok_lines)\n",
    "# Iterate through days to build daily hgs models\n",
    "for day in np.arange(0,End_Day):\n",
    "    # Day 0 model\n",
    "    if day == 0:\n",
    "        # Build text lines\n",
    "        new_cgrok_lines = CreateDailyCoupledGrokFileDay0(standalone_grok_lines,day,P)\n",
    "        # Write out\n",
    "        WriteCoupledGrokFile(new_cgrok_lines,day,cmod_hgs_dir,grok_file_stem)\n",
    "    # All other Day models\n",
    "    else:\n",
    "        # Build text lines\n",
    "        new_cgrok_lines = CreateDailyCoupledGrokFileDayN(standalone_grok_lines,day,P,grok_file_stem)\n",
    "        # Write out\n",
    "        WriteCoupledGrokFile(new_cgrok_lines,day,cmod_hgs_dir,grok_file_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateControllerBatchFile(coupled_mod_dir,coupled_mod_hgs_dir):\n",
    "    controller_path = os.path.join(coupled_mod_hgs_dir,'Controller.bat')\n",
    "    with open(controller_path,'w') as file:\n",
    "        file.write('cd {}\\ngrok > out_g.txt\\nphgs > out_h.txt\\nhsplot\\n'.format(coupled_mod_hgs_dir))\n",
    "    out_path = os.path.join(coupled_mod_hgs_dir,'out.txt')\n",
    "    with open(out_path,'w') as file:\n",
    "        file.write('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunCoupledModelHGSDaily(day,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    grok_name = grok_file_stem + 'day{}'.format(day)\n",
    "    # First, update batch.pfx\n",
    "    print('Updating batch.pfx')\n",
    "    batch_pfx_path = os.path.join(coupled_mod_hgs_dir,'batch.pfx')\n",
    "    with open(batch_pfx_path,'w') as file:\n",
    "        file.write(grok_name)\n",
    "    # Then run Controller\n",
    "    print('Running Model')\n",
    "    sp.run(['Controller.bat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_pkl_path = r'C:\\\\Users\\\\southa0000\\\\Documents\\\\HGS-DSSAT\\\\HGS-DSSAT\\\\examples\\\\lys\\\\mapping\\\\lys_mapping.p'\n",
    "rz_node_order_file_path = r'C:\\\\Users\\\\southa0000\\\\Documents\\\\HGS-DSSAT\\\\HGS-DSSAT\\\\examples\\\\lys\\\\hgs\\\\rz_node_order.txt'\n",
    "full_node_order_file_path = r'C:\\\\Users\\\\southa0000\\\\Documents\\\\HGS-DSSAT\\\\HGS-DSSAT\\\\examples\\\\lys\\\\hgs\\\\full_node_order.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildETTimeValueTable(mapping_pkl_path,rz_node_order_file_path,coupled_mod_dssat_dir,coupled_mod_hgs_dir):\n",
    "    # Load node list\n",
    "    with open(rz_node_order_file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    nodes = [int(x.strip()) for x in lines]\n",
    "    # Load mapping pickle\n",
    "    with open(mapping_pkl_path,'rb') as file:\n",
    "        map_dict = pkl.load(file)\n",
    "    # Blank list for vals\n",
    "    vals = []\n",
    "    # Iterate through nodes\n",
    "    for node in nodes:\n",
    "        # Get DSSAT location information\n",
    "        dssat_model,sheet,area_stat = map_dict[node]\n",
    "        # Get surface ET and half of soil layer 1 for sheet 0\n",
    "        if sheet == 0:\n",
    "            # Identify path to DSSAT Surface ET file\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SurfaceET.csv')\n",
    "            # Grab val up from surface file\n",
    "            valup = pd.read_csv(dssat_data_path)['EOAA'].values[0]\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valdn = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet + 1)].values[0]\n",
    "            # Set value to full surface ET + 1/2 of layer 1 ET\n",
    "            val = (valup + (0.5*valdn)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # For bottom node sheet, just get half of last DSSAT layer\n",
    "        elif sheet == 10:\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valup = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet)].values[0]\n",
    "            # Set value to 1/2 of layer 10 ET\n",
    "            val = ((0.5*valup)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # For all other sheets, take half of layer above and half of layer below\n",
    "        else:\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valup = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet)].values[0]\n",
    "            # Grab val down from soil file layer 1\n",
    "            valdn = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet + 1)].values[0]\n",
    "            # Set value to 1/2 of layer n ET and 1/2 of layer n+1 ET\n",
    "            val = ((0.5*valdn) + (0.5*valup)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # Convert DSSAT total mm to HGS total m3 and then multiply by minutes in a day to force all to be taken out in first minute of day\n",
    "        vals.append(val)\n",
    "    # Write out nflux.txt file\n",
    "    nflux_path = os.path.join(coupled_mod_hgs_dir,'nflux.txt')\n",
    "    with open(nflux_path,'w') as file:\n",
    "        file.write(str(len(vals))+'\\n')\n",
    "        lines = []\n",
    "        for val in vals:\n",
    "            lines.append(str(val)+'\\n')\n",
    "        lines[-1] = lines[-1][:-1]\n",
    "        for line in lines:\n",
    "            file.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSoluteICFile(mapping_pkl_path,rz_node_order_file_path,full_node_order_file_path,coupled_mod_dssat_dir,coupled_mod_hgs_dir,grok_file_stem,day):\n",
    "    # Load rz node list\n",
    "    with open(rz_node_order_file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    rz_nodes = [int(x.strip()) for x in lines]\n",
    "    # Load full node list\n",
    "    with open(full_node_order_file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    full_nodes = [int(x.strip()) for x in lines]\n",
    "    # Get list of non-coupled nodes\n",
    "    hgs_only_nodes = [x for x in full_nodes if not (x in rz_nodes)]\n",
    "    # Load mapping pickle\n",
    "    with open(mapping_pkl_path,'rb') as file:\n",
    "        map_dict = pkl.load(file)\n",
    "    # Blank list for vals\n",
    "    vals = []\n",
    "    ## Load yesterdays HGS NH3 and NO4 values\n",
    "    # Get pm file path\n",
    "    pm_file = grok_file_stem.split('_')[0] + '_eday{}o.pm.dat'.format(day-1)\n",
    "    pm_file_path = os.path.join(coupled_mod_hgs_dir,pm_file)\n",
    "    # Load lines\n",
    "    with open(pm_file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Get last entry in file for solution time = 1\n",
    "    start = [i for i, line in enumerate(lines) if 'SOLUTIONTIME=1.00000000000000' in line][0]\n",
    "    # Get indexes for starts and ends of no3 and nh4 sections\n",
    "    st1lines = lines[start:]\n",
    "    start_no3 = st1lines.index('# NO3\\n')\n",
    "    sub_lines = st1lines[start_no3:]\n",
    "    inds = [i for i, line in enumerate(sub_lines) if '#' in line]\n",
    "    no3lines = sub_lines[inds[0]+1:inds[1]]\n",
    "    nh4lines = sub_lines[inds[1]+1:inds[2]]\n",
    "    hgs_no3_vals = []\n",
    "    for line in no3lines:\n",
    "        for num in line.strip().split():\n",
    "            hgs_no3_vals.append(float(num))\n",
    "    hgs_nh4_vals = []\n",
    "    for line in nh4lines:\n",
    "        for num in line.strip().split():\n",
    "            hgs_nh4_vals.append(float(num))\n",
    "    ## Load dssat vals\n",
    "    # Get list of possible dssat model names\n",
    "    dssat_models = set([map_dict[x][0] for x in map_dict.keys()])\n",
    "    # Store dssat vals in dictionary\n",
    "    dssat_no3_vals = {}\n",
    "    for mdl in dssat_models:\n",
    "        dssat_output_path = os.path.join(coupled_mod_dssat_dir,str(mdl) + '_SoilNO3.csv')\n",
    "        dssat_no3_vals[mdl] = pd.read_csv(dssat_output_path)\n",
    "    dssat_nh4_vals = {}\n",
    "    for mdl in dssat_models:\n",
    "        dssat_output_path = os.path.join(coupled_mod_dssat_dir,str(mdl) + '_SoilNH4.csv')\n",
    "        dssat_nh4_vals[mdl] = pd.read_csv(dssat_output_path)\n",
    "    ## Iterate through nodes and store values from correct source in list\n",
    "    no3_vals = []\n",
    "    for node in hgs_only_nodes:\n",
    "        # Get concentration from hgs_vals and append to list\n",
    "        no3_vals.append(hgs_no3_vals[-1*node])\n",
    "    for node in rz_nodes:\n",
    "        # Identify model and sheet number\n",
    "        mdl,sheet,a_stat = map_dict[node]\n",
    "        # Get concentration from dssat_vals, convert and append to list\n",
    "        if sheet == 0:\n",
    "            no3_vals.append(dssat_no3_vals[mdl].loc[0,'NI{}D'.format(sheet+1)]*1000.)\n",
    "        elif sheet == 10:\n",
    "            no3_vals.append(dssat_no3_vals[mdl].loc[0,'NI{}'.format(sheet)]*1000.)\n",
    "        elif sheet == 9:\n",
    "            no3_vals.append(0.5*(dssat_no3_vals[mdl].loc[0,'NI{}D'.format(sheet)]*1000.)+0.5*(dssat_no3_vals[mdl].loc[0,'NI{}'.format(sheet+1)]*1000.))\n",
    "        else:\n",
    "            no3_vals.append(0.5*(dssat_no3_vals[mdl].loc[0,'NI{}D'.format(sheet)]*1000.)+0.5*(dssat_no3_vals[mdl].loc[0,'NI{}D'.format(sheet+1)]*1000.))\n",
    "    nh4_vals = []\n",
    "    for node in hgs_only_nodes:\n",
    "        # Get concentration from hgs_vals and append to list\n",
    "        nh4_vals.append(hgs_nh4_vals[-1*node])\n",
    "    for node in rz_nodes:\n",
    "        # Identify model and sheet number\n",
    "        mdl,sheet,a_stat = map_dict[node]\n",
    "        # Get concentration from dssat_vals, convert and append to list\n",
    "        if sheet == 0:\n",
    "            nh4_vals.append(dssat_nh4_vals[mdl].loc[0,'NH{}D'.format(sheet+1)]*1000.)\n",
    "        elif sheet == 10:\n",
    "            nh4_vals.append(dssat_nh4_vals[mdl].loc[0,'NH{}'.format(sheet)]*1000.)\n",
    "        elif sheet == 9:\n",
    "            nh4_vals.append(0.5*(dssat_nh4_vals[mdl].loc[0,'NH{}D'.format(sheet)]*1000.)+0.5*(dssat_nh4_vals[mdl].loc[0,'NH{}'.format(sheet+1)]*1000.))\n",
    "        else:\n",
    "            nh4_vals.append(0.5*(dssat_nh4_vals[mdl].loc[0,'NH{}D'.format(sheet)]*1000.)+0.5*(dssat_nh4_vals[mdl].loc[0,'NH{}D'.format(sheet+1)]*1000.))\n",
    "    # Write out to file\n",
    "    iconc_path = os.path.join(coupled_mod_hgs_dir,'iconc.txt')\n",
    "    with open(iconc_path,'w') as file:\n",
    "        lines = []\n",
    "        for val in no3_vals:\n",
    "            lines.append(str(val)+'\\n')\n",
    "        for val in nh4_vals:\n",
    "            lines.append(str(val)+'\\n')\n",
    "        for line in lines:\n",
    "            file.write(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 1\n",
    "BuildSoluteICFile(mapping_pkl_path,rz_node_order_file_path,full_node_order_file_path,cmod_dssat_dir,cmod_hgs_dir,grok_file_stem,day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering day 0\n",
      "Updating batch.pfx\n",
      "Running Model\n",
      "Entering day 1\n",
      "Creating nflux file for DSSAT ET\n",
      "Updating batch.pfx\n",
      "Running Model\n"
     ]
    }
   ],
   "source": [
    "## Run Daily models\n",
    "# First, create controller batch file\n",
    "CreateControllerBatchFile(cmod_dir,cmod_hgs_dir)\n",
    "# Change to coupled model directory\n",
    "os.chdir(cmod_hgs_dir)\n",
    "# Iterate through days to run daily hgs models\n",
    "for day in np.arange(0,2):\n",
    "    print('Entering day ' + str(day))\n",
    "    if day > 0:\n",
    "        print('Creating nflux file for DSSAT ET')\n",
    "        BuildETTimeValueTable(mapping_pkl_path,rz_node_order_file_path,cmod_dssat_dir,cmod_hgs_dir)\n",
    "        BuildSoluteICFile(mapping_pkl_path,rz_node_order_file_path,full_node_order_file_path,cmod_dssat_dir,cmod_hgs_dir,grok_file_stem,day)\n",
    "    RunCoupledModelHGSDaily(day,cmod_hgs_dir,grok_file_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
