{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "import pickle as pkl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dir = r'C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgs_mod_dir = os.path.join(mod_dir,'hgs')\n",
    "model_name = 'lys'\n",
    "grok_file_path = os.path.join(hgs_mod_dir,model_name + '_e.grok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_file_stem = model_name + '_e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCoupledModelDir(mod_dir):\n",
    "    # Create directories if they don't exist\n",
    "    coupled_mod_dir = os.path.join(mod_dir,'coupled')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_dir)\n",
    "    except:\n",
    "        print(coupled_mod_dir + ' already exists')\n",
    "    # Create hgs subdirectory\n",
    "    coupled_mod_hgs_dir = os.path.join(coupled_mod_dir,'hgs')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_hgs_dir)\n",
    "    except:\n",
    "        print(coupled_mod_hgs_dir + ' already exists')\n",
    "    # Create dssat subdirectory\n",
    "    coupled_mod_dssat_dir = os.path.join(coupled_mod_dir,'dssat')\n",
    "    try:\n",
    "        os.mkdir(coupled_mod_dssat_dir)\n",
    "    except:\n",
    "        print(coupled_mod_dssat_dir + ' already exists')\n",
    "    return coupled_mod_dir,coupled_mod_hgs_dir,coupled_mod_dssat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1055243466.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def GetSpinUpHeadsOutputFile(hgs_mod_dir,grok_file_stem):\n",
    "    # Identify spin up grok name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStandaloneGrokLines(grok_file_path):\n",
    "    # Read grok file\n",
    "    with open(grok_file_path,'r') as file_in:\n",
    "        lines = file_in.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetStandaloneGrokPrecSeries(standalone_grok_lines):\n",
    "    # Get start index\n",
    "    start = standalone_grok_lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    end = standalone_grok_lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Get P Block\n",
    "    plines = standalone_grok_lines[start:end]\n",
    "    # Get p series\n",
    "    start = plines.index('    time value table\\n')\n",
    "    end = plines.index('    end\\n')\n",
    "    # Get time series lines\n",
    "    tslines = plines[start+1:end]\n",
    "    # Get Daily P Series\n",
    "    p = [float(x.split(' ')[5].strip()) for x in tslines]\n",
    "    # Get end day\n",
    "    end_day = len(p)\n",
    "    return p, end_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDailyCoupledGrokFileDay0(lines,day,p):\n",
    "    ## P Section\n",
    "    # Get start index\n",
    "    pstart = lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    pend = lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Build P entry\n",
    "    pentry = f'    time value table\\n    0.0 {P[day]:.2f}\\n    end\\n'\n",
    "    ## Output Section\n",
    "    # Get start index\n",
    "    ostart = lines.index('!!--Begin Output Times Section--\\n')\n",
    "    # Get end index\n",
    "    oend = lines.index('!!--End Output Times Section--\\n')\n",
    "    new_lines = lines[:pstart+1]+[pentry]+lines[pend:ostart+1]+['1.0\\nend\\n']+lines[oend:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDailyCoupledGrokFileDayN(lines,day,p,grok_file_stem):\n",
    "    ## IC Section\n",
    "    # Get start index\n",
    "    icstart = lines.index('!!--Begin Initial Head Section--\\n')\n",
    "    # Get end index\n",
    "    icend = lines.index('!!--End Initial Head Section--\\n')\n",
    "    # Build IC\n",
    "    icentry = '! Set initial heads from day n-1\\nchoose nodes all\\n\\ninitial head from output file\\n{0}day{1}o.head_pm.0001\\n\\nclear chosen nodes\\n'.format(grok_file_stem,day-1)\n",
    "    ## Flux Nodal Section\n",
    "    # Get start index\n",
    "    fnstart = lines.index('!!--Begin Flux Nodal for DSSAT ET Section--\\n')\n",
    "    # Get end index\n",
    "    fnend = lines.index('!!--End Flux Nodal for DSSAT ET Section--\\n')\n",
    "    # Build FN\n",
    "    fnentry = '! Set flux nodal to force DSSAT ET\\nboundary condition\\n    type\\n    flux nodal\\n\\n    node set\\n    coupled_section\\n\\n    time file table\\n    0.0 nflux.txt\\n    0.00069444 none\\n    end\\nend\\n'\n",
    "    ## P Section\n",
    "    # Get start index\n",
    "    pstart = lines.index('!!--Begin Precipitation Time Series Section--\\n')\n",
    "    # Get end index\n",
    "    pend = lines.index('!!--End Precipitation Time Series Section--\\n')\n",
    "    # Build P entry\n",
    "    pentry = f'    time value table\\n    0.0 {p[day]:.2f}\\n    end\\n'\n",
    "    ## Output Section\n",
    "    # Get start index\n",
    "    ostart = lines.index('!!--Begin Output Times Section--\\n')\n",
    "    # Get end index\n",
    "    oend = lines.index('!!--End Output Times Section--\\n')\n",
    "    new_lines = lines[:icstart+1]+[icentry]+lines[icend:fnstart+1]+[fnentry]+lines[fnend:pstart+1]+[pentry]+lines[pend:ostart+1]+['1.0\\nend\\n']+lines[oend:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteCoupledGrokFile(new_lines,day,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    new_grok_name = grok_file_stem + 'day{}'.format(day) + '.grok'\n",
    "    new_grok_path = os.path.join(coupled_mod_hgs_dir,new_grok_name)\n",
    "    with open(new_grok_path,'w') as file:\n",
    "        for entry in new_lines:\n",
    "            file.write(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled already exists\n",
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled\\hgs already exists\n",
      "C:\\Users\\southa0000\\Documents\\HGS-DSSAT\\HGS-DSSAT\\examples\\lys\\coupled\\dssat already exists\n"
     ]
    }
   ],
   "source": [
    "## Build Coupled Model\n",
    "# Create Directory Structure\n",
    "cmod_dir,cmod_hgs_dir,cmod_dssat_dir = CreateCoupledModelDir(mod_dir)\n",
    "# Get standalone model grok lines and Prec series\n",
    "standalone_grok_lines = GetStandaloneGrokLines(grok_file_path)\n",
    "P, End_Day = GetStandaloneGrokPrecSeries(standalone_grok_lines)\n",
    "# Iterate through days to build daily hgs models\n",
    "for day in np.arange(0,End_Day):\n",
    "    # Day 0 model\n",
    "    if day == 0:\n",
    "        # Build text lines\n",
    "        new_cgrok_lines = CreateDailyCoupledGrokFileDay0(standalone_grok_lines,day,P)\n",
    "        # Write out\n",
    "        WriteCoupledGrokFile(new_cgrok_lines,day,cmod_hgs_dir,grok_file_stem)\n",
    "    # All other Day models\n",
    "    else:\n",
    "        # Build text lines\n",
    "        new_cgrok_lines = CreateDailyCoupledGrokFileDayN(standalone_grok_lines,day,P,grok_file_stem)\n",
    "        # Write out\n",
    "        WriteCoupledGrokFile(new_cgrok_lines,day,cmod_hgs_dir,grok_file_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateControllerBatchFile(coupled_mod_dir,coupled_mod_hgs_dir):\n",
    "    controller_path = os.path.join(coupled_mod_hgs_dir,'Controller.bat')\n",
    "    with open(controller_path,'w') as file:\n",
    "        file.write('cd {}\\ngrok > out_g.txt\\nphgs > out_h.txt\\n'.format(coupled_mod_hgs_dir))\n",
    "    out_path = os.path.join(coupled_mod_hgs_dir,'out.txt')\n",
    "    with open(out_path,'w') as file:\n",
    "        file.write('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunCoupledModelHGSDaily(day,coupled_mod_hgs_dir,grok_file_stem):\n",
    "    grok_name = grok_file_stem + 'day{}'.format(day)\n",
    "    # First, update batch.pfx\n",
    "    print('Updating batch.pfx')\n",
    "    batch_pfx_path = os.path.join(coupled_mod_hgs_dir,'batch.pfx')\n",
    "    with open(batch_pfx_path,'w') as file:\n",
    "        file.write(grok_name)\n",
    "    # Then run Controller\n",
    "    print('Running Model')\n",
    "    sp.run(['Controller.bat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_pkl_path = r'C:\\\\Users\\\\southa0000\\\\Documents\\\\HGS-DSSAT\\\\HGS-DSSAT\\\\examples\\\\lys\\\\mapping\\\\lys_mapping.p'\n",
    "node_order_file_path = r'C:\\\\Users\\\\southa0000\\\\Documents\\\\HGS-DSSAT\\\\HGS-DSSAT\\\\examples\\\\lys\\\\hgs\\\\node_order.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildETTimeValueTable(mapping_pkl_path,node_order_file_path,coupled_mod_dssat_dir,coupled_mod_hgs_dir):\n",
    "    # Load node list\n",
    "    with open(node_order_file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    nodes = [int(x.strip()) for x in lines]\n",
    "    # Load mapping pickle\n",
    "    with open(mapping_pkl_path,'rb') as file:\n",
    "        map_dict = pkl.load(file)\n",
    "    # Blank list for vals\n",
    "    vals = []\n",
    "    # Iterate through nodes\n",
    "    for node in nodes:\n",
    "        # Get DSSAT location information\n",
    "        dssat_model,sheet,area_stat = map_dict[node]\n",
    "        # Get surface ET and half of soil layer 1 for sheet 0\n",
    "        if sheet == 0:\n",
    "            # Identify path to DSSAT Surface ET file\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SurfaceET.csv')\n",
    "            # Grab val up from surface file\n",
    "            valup = pd.read_csv(dssat_data_path)['EOAA'].values[0]\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valdn = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet + 1)].values[0]\n",
    "            # Set value to full surface ET + 1/2 of layer 1 ET\n",
    "            val = (valup + (0.5*valdn)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # For bottom node sheet, just get half of last DSSAT layer\n",
    "        elif sheet == 10:\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valup = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet)].values[0]\n",
    "            # Set value to 1/2 of layer 10 ET\n",
    "            val = ((0.5*valup)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # For all other sheets, take half of layer above and half of layer below\n",
    "        else:\n",
    "            # Identify path to DSSAT Soil ET File\n",
    "            dssat_data_path = os.path.join(coupled_mod_dssat_dir,str(dssat_model) + '_SoilET.csv')\n",
    "            # Grab val down from soil file layer 1\n",
    "            valup = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet)].values[0]\n",
    "            # Grab val down from soil file layer 1\n",
    "            valdn = pd.read_csv(dssat_data_path)['ES{}D'.format(sheet + 1)].values[0]\n",
    "            # Set value to 1/2 of layer n ET and 1/2 of layer n+1 ET\n",
    "            val = ((0.5*valdn) + (0.5*valup)) * -1 * 24. * 60. / 1000. * (1./area_stat)\n",
    "        # Convert DSSAT total mm to HGS total m3 and then multiply by minutes in a day to force all to be taken out in first minute of day\n",
    "        vals.append(val)\n",
    "    # Write out nflux.txt file\n",
    "    nflux_path = os.path.join(coupled_mod_hgs_dir,'nflux.txt')\n",
    "    with open(nflux_path,'w') as file:\n",
    "        file.write(str(len(vals))+'\\n')\n",
    "        lines = []\n",
    "        for val in vals:\n",
    "            lines.append(str(val)+'\\n')\n",
    "        lines[-1] = lines[-1][:-1]\n",
    "        for line in lines:\n",
    "            file.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering day 0\n",
      "Updating batch.pfx\n",
      "Running Model\n",
      "Entering day 1\n",
      "Creating nflux file for DSSAT ET\n",
      "Updating batch.pfx\n",
      "Running Model\n"
     ]
    }
   ],
   "source": [
    "## Run Daily models\n",
    "# First, create controller batch file\n",
    "CreateControllerBatchFile(cmod_dir,cmod_hgs_dir)\n",
    "# Change to coupled model directory\n",
    "os.chdir(cmod_hgs_dir)\n",
    "# Iterate through days to run daily hgs models\n",
    "for day in np.arange(0,2):\n",
    "    print('Entering day ' + str(day))\n",
    "    if day > 0:\n",
    "        print('Creating nflux file for DSSAT ET')\n",
    "        BuildETTimeValueTable(mapping_pkl_path,node_order_file_path,cmod_dssat_dir,cmod_hgs_dir)\n",
    "    RunCoupledModelHGSDaily(day,cmod_hgs_dir,grok_file_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
